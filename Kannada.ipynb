{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEyEcB2A4dmA"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "from sklearn.metrics import accuracy_score,classification_report, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uS6kc4z_8qa"
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'ner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGgxWSNw42U2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f42629f-c5d1-42ca-d9db-3b8291564602"
      },
      "source": [
        "# DATA CLEANING AND PREPARATION #\n",
        "class Utils(object):\n",
        "\n",
        "    def cleanText(self, text):\n",
        "        review = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\", \" \", str(text))\n",
        "        review = re.sub(r\"\\([\\s\\S]*\\)\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\", \" \", str(review))\n",
        "        review = review.lower()\n",
        "        review = re.sub(r\"that's\", \"that is\", str(review))\n",
        "        review = re.sub(r\"there's\", \"there is\", str(review))\n",
        "        review = re.sub(r\"what's\", \"what is\", str(review))\n",
        "        review = re.sub(r\"where's\", \"where is\", str(review))\n",
        "        review = re.sub(r\"it's\", \"it is\", str(review))\n",
        "        review = re.sub(r\"who's\", \"who is\", str(review))\n",
        "        review = re.sub(r\"i'm\", \"i am\", str(review))\n",
        "        review = re.sub(r\"she's\", \"she is\", str(review))\n",
        "        review = re.sub(r\"he's\", \"he is\", str(review))\n",
        "        review = re.sub(r\"they're\", \"they are\", str(review))\n",
        "        review = re.sub(r\"who're\", \"who are\", str(review))\n",
        "        review = re.sub(r\"ain't\", \"am not\", str(review))\n",
        "        review = re.sub(r\"wouldn't\", \"would not\", str(review))\n",
        "        review = re.sub(r\"shouldn't\", \"should not\", str(review))\n",
        "        review = re.sub(r\"can't\", \"can not\", str(review))\n",
        "        review = re.sub(r\"couldn't\", \"could not\", str(review))\n",
        "        review = re.sub(r\"won't\", \"will not\", str(review))\n",
        "        review = re.sub(r\" pm \", \" \", str(review))\n",
        "        review = re.sub(r\" am \", \" \", str(review))\n",
        "        review = re.sub(r'[^\\[\\]]+(?=\\])', \" \", str(review))\n",
        "        review = re.sub(r\"\\W\", \" \", str(review))\n",
        "        review = re.sub(r\"\\d\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+[a-z]\\s+\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+[a-z]$\", \" \", str(review))\n",
        "        review = re.sub(r\"^[a-z]\\s+\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+\", \" \", str(review))\n",
        "        return review\n",
        "\n",
        "    def remove_punc(self, text):\n",
        "        table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "        return text.translate(table)\n",
        "\n",
        "    def remove_emoticon(self, text):\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "    \n",
        "    def lemmatization(self, text):\n",
        "        doc = nlp(text)\n",
        "        return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "    nltk.download('stopwords')\n",
        "    def remove_stops(self, text):\n",
        "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "        text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "        return \" \".join(text)\n",
        "\n",
        "\n",
        "    def readData1(self, path, inputColumnIndex=0, outputColumnIndex=1):\n",
        "        df = pd.read_csv(path, error_bad_lines=False, sep='\\t')\n",
        "        X = df.iloc[:, inputColumnIndex].values\n",
        "        y = df.iloc[:, outputColumnIndex].values\n",
        "        return X,y\n",
        "    \n",
        "    def readData2(self, path, inputColumnIndex=1, outputColumnIndex=2):\n",
        "        df = pd.read_csv(path, error_bad_lines=False, sep='\\t')\n",
        "        X = df.iloc[:, inputColumnIndex].values\n",
        "        y = df.iloc[:, outputColumnIndex].values\n",
        "        return X,y\n",
        "    \n",
        "    def draw_prediction_results(self, y_pred, y_test, my_tags, method):\n",
        "        print('accuracy of ' + method + ': %s' % accuracy_score(y_pred, y_test))\n",
        "        print(classification_report(y_test, y_pred, target_names=my_tags, digits = 6))\n",
        "    \n",
        "    def create_csv(self, y_pred, y_test, classifier):\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        lines = report.split('\\n')\n",
        "        line = lines[-2].split()\n",
        "        line.remove('weighted')\n",
        "        line[0] = classifier\n",
        "        df = pd.DataFrame(line).transpose()\n",
        "        csv_data = df.to_csv(index=False)\n",
        "        df.to_csv('Kannada.csv',  mode='a', header=False, index=False)\n",
        "        \n",
        "    \n",
        "    def crossValidation(self, prediction, input, output, k=5):\n",
        "        scores = cross_val_score(prediction, input,output, cv=k)\n",
        "        print(\"Accuracy of Cross Validation Mean: %0.6f (+/- %0.6f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "727sZupe6Gxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c791c8b-f038-4a89-8f62-6d85583f0ed4"
      },
      "source": [
        "# FUNCTION CALL FOR DATA CLEANING AND PREPARATION #\n",
        "utils = Utils()\n",
        "X1,y_train=utils.readData1('kannada_sentiment_full_train.tsv')\n",
        "\n",
        "ourTags =['not-Kannada', 'unknown_state', 'Positive', 'Mixed_feelings', 'Negative']\n",
        "X_train=[]\n",
        "\n",
        "for i in range(0, len(X1)):\n",
        "    t = utils.cleanText(X1[i])\n",
        "    t = utils.remove_emoticon(t)\n",
        "    t = utils.remove_punc(t)\n",
        "    t = utils.remove_stops(t)\n",
        "    t = utils.lemmatization(t)\n",
        "    X_train.append(t)\n",
        "\n",
        "print(X_train[:10])\n",
        "#X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ಒ ದ ದ ಶದ ಮ ದ ವರ ಯ ವ ದ ಅದರ ಆರ ಥ ಕ ಸ ಥ ತ ಯನ ನ ಅವಲ ಬ ಸ ವ ದ ಲ ಲ ಅವರ ಮ ನವ ಯತ ಯಲ ಲ ಎಷ ಟ ಸಮರ ಥರ ಎನ ನ ವ ದನ ನ ಅವಲ ಬ ಸ ದ ಭ ರತದಲ ಲ ಅನಕ ಷರತ ಇದ ಆ ಅನಕ ಷರಸ ಥರನ ನ ಅವರ ಅನಕ ಷರತ ಯ', 'ಕನ ನಡದಲ ಲ ಡ ಲ ಟ ಕ ಅಪ ಡ ಟ ಸ ಪಡ ಯಲ ಸಬ ಸ ಕ ರ ಬ ಮ ಡ ನಮ ಮ ಚನ ನ ಲ ಗ', 'super sar song', 'tiktoker present situation nನ ಡ ವವರ ಯ ರ ನಮ ಮ ವ ಡ ಯ ನ', 'super ಸ ಗ ವ ರ ನ ಸ', 'varshakke thagadu movie madi industry haal mado hero galu ondh kade adrenn varsha kasta pattu ondhu olle film mado namma rakshith shetty haagu yash innondh kade', 'tickets amount adru mosa illa love', 'super super super film explain', 'wild rex ಕಟ ಟಬ ಕ bronಖ ಡ ತ ಕಟ ಟ ತ ತ bro', 'shankaragouda desaigoudra super']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPsszKRGni1Z"
      },
      "source": [
        "# FUNCTION CALL FOR DATA CLEANING AND PREPARATION #\n",
        "utils = Utils()\n",
        "X2,y_test=utils.readData2('kannada_sentiment_full_test_withlabels.tsv')\n",
        "\n",
        "X_test=[]\n",
        "\n",
        "for i in range(0, len(X2)):\n",
        "    t = utils.cleanText(X2[i])\n",
        "    t = utils.remove_emoticon(t)\n",
        "    t = utils.remove_punc(t)\n",
        "    t = utils.remove_stops(t)\n",
        "    t = utils.lemmatization(t)\n",
        "    X_test.append(t)\n",
        "#print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqPVciOg-K5c"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhcrBCgJLDub"
      },
      "source": [
        "line = ['', 'precision', 'recall', 'f1-score',  'support']\n",
        "df = pd.DataFrame(line).transpose()\n",
        "csv_data = df.to_csv(index=False)\n",
        "df.to_csv('Kannada.csv',  mode='a', header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODSXTCHsLd8Q",
        "outputId": "55a985cc-1e49-45f7-9918-095ba3e406f6"
      },
      "source": [
        "# LOGISTIC REGRESSION #\n",
        "lrp = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2,analyzer='word', ngram_range=(1, 3))),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('lr', LogisticRegression(max_iter=1000))\n",
        "                ])\n",
        "lrp.fit(X_train, y_train)\n",
        "y_pred = lrp.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'LR')\n",
        "\n",
        "#prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Logistic Regression\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Logistic Regression: 0.6158854166666666\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.444444  0.061538  0.108108        65\n",
            " unknown_state   0.642857  0.573248  0.606061       157\n",
            "      Positive   0.627490  0.842246  0.719178       374\n",
            "Mixed_feelings   0.597701  0.472727  0.527919       110\n",
            "      Negative   0.400000  0.193548  0.260870        62\n",
            "\n",
            "      accuracy                       0.615885       768\n",
            "     macro avg   0.542499  0.428662  0.444427       768\n",
            "  weighted avg   0.592508  0.615885  0.579943       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGB5YURAThfx",
        "outputId": "3a2d84c6-cc8b-46a5-f038-5bce51734f8d"
      },
      "source": [
        "# MULTINOMIAL NAIVE BAYES #\n",
        "multinomial_naive_bayes = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('multinomial_naive_bayes',\n",
        "                         MultinomialNB())\n",
        "                        ])\n",
        "multinomial_naive_bayes.fit(X_train, y_train)\n",
        "y_pred = multinomial_naive_bayes.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'MNB')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Multinomial Naive Bayes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Multinomial Naive Bayes: 0.6171875\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.400000  0.030769  0.057143        65\n",
            " unknown_state   0.728070  0.528662  0.612546       157\n",
            "      Positive   0.602862  0.901070  0.722401       374\n",
            "Mixed_feelings   0.611111  0.400000  0.483516       110\n",
            "      Negative   0.444444  0.129032  0.200000        62\n",
            "\n",
            "      accuracy                       0.617188       768\n",
            "     macro avg   0.557298  0.397907  0.415121       768\n",
            "  weighted avg   0.599681  0.617188  0.567251       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22IIDKIOwTaK",
        "outputId": "86594387-5184-456d-d1f8-e23a892dcf91"
      },
      "source": [
        "# LINEAR SVM #\n",
        "linear_svm = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('linear_svc',\n",
        "                        SVC(kernel='linear'))\n",
        "                        ])\n",
        "linear_svm.fit(X_train, y_train)\n",
        "y_pred = linear_svm.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'L-SVM')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Linear SVM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Linear SVM: 0.6041666666666666\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.312500  0.076923  0.123457        65\n",
            " unknown_state   0.611111  0.560510  0.584718       157\n",
            "      Positive   0.640167  0.818182  0.718310       374\n",
            "Mixed_feelings   0.556701  0.490909  0.521739       110\n",
            "      Negative   0.333333  0.177419  0.231579        62\n",
            "\n",
            "      accuracy                       0.604167       768\n",
            "     macro avg   0.490763  0.424789  0.435960       768\n",
            "  weighted avg   0.569770  0.604167  0.573206       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1cYONgUxcBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e083ed-ac29-457a-9dde-ee20ead3bc20"
      },
      "source": [
        "# RBF SVM #\n",
        "rbf_svm = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('rbf_svc',\n",
        "                        SVC(kernel='rbf', gamma=1))\n",
        "                        ])\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "y_pred = rbf_svm.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'R-SVM')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"RBF SVM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of RBF SVM: 0.6158854166666666\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.428571  0.046154  0.083333        65\n",
            " unknown_state   0.675439  0.490446  0.568266       157\n",
            "      Positive   0.629703  0.850267  0.723549       374\n",
            "Mixed_feelings   0.619565  0.518182  0.564356       110\n",
            "      Negative   0.360000  0.290323  0.321429        62\n",
            "\n",
            "      accuracy                       0.615885       768\n",
            "     macro avg   0.542656  0.439074  0.452187       768\n",
            "  weighted avg   0.598805  0.615885  0.582356       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-ucCcstztRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb11714-7e7a-4cf7-b48b-1dd102d52aa0"
      },
      "source": [
        "# POLY SVM #\n",
        "poly_svm = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('poly_svc',\n",
        "                        SVC(kernel='poly',degree = 1))\n",
        "                        ])\n",
        "poly_svm.fit(X_train, y_train)\n",
        "y_pred = poly_svm.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'P-SVM')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"POLY SVM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of POLY SVM: 0.60546875\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.294118  0.076923  0.121951        65\n",
            " unknown_state   0.618056  0.566879  0.591362       157\n",
            "      Positive   0.640167  0.818182  0.718310       374\n",
            "Mixed_feelings   0.556701  0.490909  0.521739       110\n",
            "      Negative   0.343750  0.177419  0.234043        62\n",
            "\n",
            "      accuracy                       0.605469       768\n",
            "     macro avg   0.490558  0.426062  0.437481       768\n",
            "  weighted avg   0.570475  0.605469  0.574636       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5ovnOnrv6-Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a26e6040-8c97-47a2-9b50-b738939bfded"
      },
      "source": [
        "# RANDOM FOREST #\n",
        "random_forest = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('random_forest',\n",
        "                         RandomForestClassifier())\n",
        "                        ])\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_pred = random_forest.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'RF')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Random Forest\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Random Forest: 0.5833333333333334\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.187500  0.046154  0.074074        65\n",
            " unknown_state   0.675000  0.515924  0.584838       157\n",
            "      Positive   0.659674  0.756684  0.704857       374\n",
            "Mixed_feelings   0.513043  0.536364  0.524444       110\n",
            "      Negative   0.250000  0.354839  0.293333        62\n",
            "\n",
            "      accuracy                       0.583333       768\n",
            "     macro avg   0.457043  0.441993  0.436309       768\n",
            "  weighted avg   0.568770  0.583333  0.567873       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s31N5aEL8rIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b796c85-89e9-4b0f-8f84-b951540b09f0"
      },
      "source": [
        "# KNeighborsClassifier #\n",
        "knn = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('kNN', KNeighborsClassifier(n_neighbors=3))\n",
        "                        ])\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'kNN')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"KNeighborsClassifier\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of KNeighborsClassifier: 0.3776041666666667\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.142857  0.138462  0.140625        65\n",
            " unknown_state   0.621212  0.261146  0.367713       157\n",
            "      Positive   0.725118  0.409091  0.523077       374\n",
            "Mixed_feelings   0.191816  0.681818  0.299401       110\n",
            "      Negative   0.324324  0.193548  0.242424        62\n",
            "\n",
            "      accuracy                       0.377604       768\n",
            "     macro avg   0.401066  0.336813  0.314648       768\n",
            "  weighted avg   0.545857  0.377604  0.404254       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc8SBEBsMeo8"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import StackingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm7FWo_vNqbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae11e465-b6c5-4bf5-dddc-75f2d393738f"
      },
      "source": [
        "# EXTRA TREE CLASSIFIER #\n",
        "extra_tree = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('extra tree',\n",
        "                         ExtraTreesClassifier())\n",
        "                        ])\n",
        "extra_tree.fit(X_train, y_train)\n",
        "y_pred = extra_tree.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'XTree')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Extra Tree Classifier\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Extra Tree Classifier: 0.5846354166666666\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.137931  0.061538  0.085106        65\n",
            " unknown_state   0.679688  0.554140  0.610526       157\n",
            "      Positive   0.674699  0.748663  0.709759       374\n",
            "Mixed_feelings   0.521739  0.545455  0.533333       110\n",
            "      Negative   0.222222  0.290323  0.251748        62\n",
            "\n",
            "      accuracy                       0.584635       768\n",
            "     macro avg   0.447256  0.440024  0.438095       768\n",
            "  weighted avg   0.571853  0.584635  0.574361       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCP0liD9P5Mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d64028c-b759-4a73-8534-72190684a8f3"
      },
      "source": [
        "# VOTING CLASSIFIER #\n",
        "# ALL CLASSIFIERS #\n",
        "\n",
        "#create a dictionary of our models\n",
        "estimators=[(\"Linear SVM\", linear_svm), (\"Logistic Regression\", lrp), (\"Multinomial Naive Bayes\", multinomial_naive_bayes), (\"Extra Tree\", extra_tree), (\"Random Forest\", random_forest), (\"Poly SVM\", poly_svm), (\"RBF SVM\", rbf_svm), (\"KNeighborsClassifier\", knn)]\n",
        "\n",
        "hard_ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "hard_ensemble.fit(X_train, y_train)\n",
        "y_pred = hard_ensemble.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'HEns')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Hard Ensemble\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Hard Ensemble: 0.6119791666666666\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.300000  0.046154  0.080000        65\n",
            " unknown_state   0.649254  0.554140  0.597938       157\n",
            "      Positive   0.621302  0.842246  0.715096       374\n",
            "Mixed_feelings   0.580645  0.490909  0.532020       110\n",
            "      Negative   0.458333  0.177419  0.255814        62\n",
            "\n",
            "      accuracy                       0.611979       768\n",
            "     macro avg   0.521907  0.422174  0.436174       768\n",
            "  weighted avg   0.580843  0.611979  0.574095       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtugxjUBOlbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb754fe3-b749-4688-acab-ac84ab88f546"
      },
      "source": [
        "# VOTING CLASSIFIER #\n",
        "# TOP 5 CLASSIFIERS #\n",
        "\n",
        "#create a dictionary of our models\n",
        "estimators=[(\"Linear SVM\", linear_svm), (\"Logistic Regression\", lrp),  (\"Random Forest\", random_forest), (\"Poly SVM\", poly_svm), (\"RBF SVM\", rbf_svm)]\n",
        "\n",
        "hard_ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "hard_ensemble.fit(X_train, y_train)\n",
        "y_pred = hard_ensemble.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'HEns5')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Hard Ensemble\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Hard Ensemble: 0.6080729166666666\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.384615  0.076923  0.128205        65\n",
            " unknown_state   0.630435  0.554140  0.589831       157\n",
            "      Positive   0.628283  0.831551  0.715765       374\n",
            "Mixed_feelings   0.569892  0.481818  0.522167       110\n",
            "      Negative   0.379310  0.177419  0.241758        62\n",
            "\n",
            "      accuracy                       0.608073       768\n",
            "     macro avg   0.518507  0.424370  0.439545       768\n",
            "  weighted avg   0.579637  0.608073  0.574297       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2WVBt7tQ0MA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c31989dd-d1fe-4f31-9315-a0609c883378"
      },
      "source": [
        "# VOTING CLASSIFIER #\n",
        "# TOP 3 CLASSIFIERS #\n",
        "\n",
        "#create a dictionary of our models\n",
        "estimators=[(\"Linear SVM\", linear_svm), (\"Logistic Regression\", lrp), (\"Poly SVM\", poly_svm)]\n",
        "\n",
        "hard_ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "hard_ensemble.fit(X_train, y_train)\n",
        "y_pred = hard_ensemble.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'HEns3')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Hard Ensemble\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Hard Ensemble: 0.60546875\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.294118  0.076923  0.121951        65\n",
            " unknown_state   0.613793  0.566879  0.589404       157\n",
            "      Positive   0.641509  0.818182  0.719154       374\n",
            "Mixed_feelings   0.556701  0.490909  0.521739       110\n",
            "      Negative   0.343750  0.177419  0.234043        62\n",
            "\n",
            "      accuracy                       0.605469       768\n",
            "     macro avg   0.489974  0.426062  0.437258       768\n",
            "  weighted avg   0.570257  0.605469  0.574647       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tevtGnJ6RIYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "806f0f45-2390-41ab-8355-c72e61181ee4"
      },
      "source": [
        "# VOTING CLASSIFIER #\n",
        "# BEST OF ALL CLASSIFIERS #\n",
        "\n",
        "#create a dictionary of our models\n",
        "estimators=[(\"Linear SVM\", linear_svm), (\"Logistic Regression\", lrp), (\"Multinomial Naive Bayes\", multinomial_naive_bayes), (\"Extra Tree\", extra_tree)]\n",
        "\n",
        "hard_ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "hard_ensemble.fit(X_train, y_train)\n",
        "y_pred = hard_ensemble.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'HEnsA')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Hard Ensemble\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Hard Ensemble: 0.6158854166666666\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.444444  0.061538  0.108108        65\n",
            " unknown_state   0.642857  0.573248  0.606061       157\n",
            "      Positive   0.625247  0.847594  0.719637       374\n",
            "Mixed_feelings   0.590909  0.472727  0.525253       110\n",
            "      Negative   0.416667  0.161290  0.232558        62\n",
            "\n",
            "      accuracy                       0.615885       768\n",
            "     macro avg   0.544025  0.423280  0.438323       768\n",
            "  weighted avg   0.591788  0.615885  0.577499       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrvYzJMSSl2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ab525c2-3949-47ae-a461-81df05ba261b"
      },
      "source": [
        "# ADABOOST #\n",
        "#seed = 10\n",
        "num_trees = 25\n",
        "\n",
        "ada_boost = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('ada_boost',\n",
        "                         AdaBoostClassifier(n_estimators=num_trees))\n",
        "                        ])\n",
        "ada_boost.fit(X_train, y_train)\n",
        "y_pred = ada_boost.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'AdaB')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Ada Boost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Ada Boost: 0.5403645833333334\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.600000  0.046154  0.085714        65\n",
            " unknown_state   0.706897  0.261146  0.381395       157\n",
            "      Positive   0.537500  0.919786  0.678501       374\n",
            "Mixed_feelings   0.511628  0.200000  0.287582       110\n",
            "      Negative   0.227273  0.080645  0.119048        62\n",
            "\n",
            "      accuracy                       0.540365       768\n",
            "     macro avg   0.516659  0.301546  0.310448       768\n",
            "  weighted avg   0.548669  0.540365  0.466439       768\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai2QQGdioGlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ebca196-5b96-409a-9834-3ff897d6fdcc"
      },
      "source": [
        "# XGBOOST #\n",
        "xg_boost = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('xgboost',\n",
        "                         XGBClassifier(learning_rate =0.1,\n",
        " n_estimators=1000,\n",
        " max_depth=5,\n",
        " min_child_weight=1,\n",
        " gamma=0,\n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " nthread=4,\n",
        " scale_pos_weight=1,\n",
        " seed=27))\n",
        "])\n",
        "xg_boost.fit(X_train, y_train)\n",
        "y_pred = xg_boost.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'XGB')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"XGBoost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of XGBoost: 0.5690104166666666\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "   not-Kannada   0.259259  0.107692  0.152174        65\n",
            " unknown_state   0.622222  0.535032  0.575342       157\n",
            "      Positive   0.628009  0.767380  0.690734       374\n",
            "Mixed_feelings   0.495413  0.490909  0.493151       110\n",
            "      Negative   0.125000  0.080645  0.098039        62\n",
            "\n",
            "      accuracy                       0.569010       768\n",
            "     macro avg   0.425981  0.396332  0.401888       768\n",
            "  weighted avg   0.536017  0.569010  0.545416       768\n",
            "\n"
          ]
        }
      ]
    }
  ]
}