{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEyEcB2A4dmA"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "\n",
        "from sklearn.metrics import accuracy_score,classification_report, f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uS6kc4z_8qa"
      },
      "source": [
        "nlp = spacy.load('en', disable=['parser', 'ner'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGgxWSNw42U2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c97d71-09c6-4456-de70-f511b468c0a0"
      },
      "source": [
        "# DATA CLEANING AND PREPARATION #\n",
        "class Utils(object):\n",
        "\n",
        "    def cleanText(self, text):\n",
        "        review = re.sub(r\"^https://t.co/[a-zA-Z0-9]*\\s\", \" \", str(text))\n",
        "        review = re.sub(r\"\\([\\s\\S]*\\)\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*\\s\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+https://t.co/[a-zA-Z0-9]*$\", \" \", str(review))\n",
        "        review = review.lower()\n",
        "        review = re.sub(r\"that's\", \"that is\", str(review))\n",
        "        review = re.sub(r\"there's\", \"there is\", str(review))\n",
        "        review = re.sub(r\"what's\", \"what is\", str(review))\n",
        "        review = re.sub(r\"where's\", \"where is\", str(review))\n",
        "        review = re.sub(r\"it's\", \"it is\", str(review))\n",
        "        review = re.sub(r\"who's\", \"who is\", str(review))\n",
        "        review = re.sub(r\"i'm\", \"i am\", str(review))\n",
        "        review = re.sub(r\"she's\", \"she is\", str(review))\n",
        "        review = re.sub(r\"he's\", \"he is\", str(review))\n",
        "        review = re.sub(r\"they're\", \"they are\", str(review))\n",
        "        review = re.sub(r\"who're\", \"who are\", str(review))\n",
        "        review = re.sub(r\"ain't\", \"am not\", str(review))\n",
        "        review = re.sub(r\"wouldn't\", \"would not\", str(review))\n",
        "        review = re.sub(r\"shouldn't\", \"should not\", str(review))\n",
        "        review = re.sub(r\"can't\", \"can not\", str(review))\n",
        "        review = re.sub(r\"couldn't\", \"could not\", str(review))\n",
        "        review = re.sub(r\"won't\", \"will not\", str(review))\n",
        "        review = re.sub(r\" pm \", \" \", str(review))\n",
        "        review = re.sub(r\" am \", \" \", str(review))\n",
        "        review = re.sub(r'[^\\[\\]]+(?=\\])', \" \", str(review))\n",
        "        review = re.sub(r\"\\W\", \" \", str(review))\n",
        "        review = re.sub(r\"\\d\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+[a-z]\\s+\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+[a-z]$\", \" \", str(review))\n",
        "        review = re.sub(r\"^[a-z]\\s+\", \" \", str(review))\n",
        "        review = re.sub(r\"\\s+\", \" \", str(review))\n",
        "        return review\n",
        "\n",
        "    def remove_punc(self, text):\n",
        "        table = str.maketrans(\"\", \"\", string.punctuation)\n",
        "        return text.translate(table)\n",
        "\n",
        "    def remove_emoticon(self, text):\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "    \n",
        "    def lemmatization(self, text):\n",
        "        doc = nlp(text)\n",
        "        return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "    nltk.download('stopwords')\n",
        "    def remove_stops(self, text):\n",
        "        stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "        text = [word.lower() for word in text.split() if word.lower() not in stop_words]\n",
        "        return \" \".join(text)\n",
        "\n",
        "\n",
        "    def readData1(self, path, inputColumnIndex=0, outputColumnIndex=1):\n",
        "        df = pd.read_csv(path, error_bad_lines=False, sep='\\t')\n",
        "        X = df.iloc[:, inputColumnIndex].values\n",
        "        y = df.iloc[:, outputColumnIndex].values\n",
        "        return X,y\n",
        "    \n",
        "    def readData2(self, path, inputColumnIndex=1, outputColumnIndex=2):\n",
        "        df = pd.read_csv(path, error_bad_lines=False, sep='\\t')\n",
        "        X = df.iloc[:, inputColumnIndex].values\n",
        "        y = df.iloc[:, outputColumnIndex].values\n",
        "        return X,y\n",
        "    \n",
        "    def draw_prediction_results(self, y_pred, y_test, my_tags, method):\n",
        "        print('accuracy of ' + method + ': %s' % accuracy_score(y_pred, y_test))\n",
        "        print(classification_report(y_test, y_pred, target_names=my_tags, digits = 6))\n",
        "    \n",
        "    def create_csv(self, y_pred, y_test, classifier):\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        lines = report.split('\\n')\n",
        "        line = lines[-2].split()\n",
        "        line.remove('weighted')\n",
        "        line[0] = classifier\n",
        "        df = pd.DataFrame(line).transpose()\n",
        "        csv_data = df.to_csv(index=False)\n",
        "        df.to_csv('Tamil.csv',  mode='a', header=False, index=False)\n",
        "        \n",
        "    \n",
        "    def crossValidation(self, prediction, input, output, k=5):\n",
        "        scores = cross_val_score(prediction, input,output, cv=k)\n",
        "        print(\"Accuracy of Cross Validation Mean: %0.6f (+/- %0.6f)\" % (scores.mean(), scores.std() * 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "727sZupe6Gxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "657428e1-fdef-4f7e-c7ca-4d4f69be92a0"
      },
      "source": [
        "# FUNCTION CALL FOR DATA CLEANING AND PREPARATION #\n",
        "utils = Utils()\n",
        "X1,y_train=utils.readData1('tamil_sentiment_full_train.tsv')\n",
        "\n",
        "ourTags =['not-Tamil', 'unknown_state', 'Positive', 'Mixed_feelings', 'Negative']\n",
        "X_train=[]\n",
        "\n",
        "for i in range(0, len(X1)):\n",
        "    t = utils.cleanText(X1[i])\n",
        "    t = utils.remove_emoticon(t)\n",
        "    t = utils.remove_punc(t)\n",
        "    t = utils.remove_stops(t)\n",
        "    t = utils.lemmatization(t)\n",
        "    X_train.append(t)\n",
        "\n",
        "print(X_train[:10])\n",
        "#X_train, X_test, y_train, y_test = train_test_split(corpus, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['vani bhojam fan hit like solli like vangida vendiyathu', 'love ajith like', 'ennaya trailer ku mudi ellam nikkudhu vera level trailer', 'vijay annaa ur maassssss therrrrriiiiii', 'நம ப நட ந ச ம த ன ப ச ச', 'gommala end vera level da deii getrajinifie', 'vjs anna kaaga like potavanga like pannuga', 'theri semma theri joseph kuruvilla vijay kumar awesome kumar', 'ithu yethu maathiri illama puthu maathiyaala irukku', 'wow back baasha mode thalaivaaaa petta paraakkkkk']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPsszKRGni1Z"
      },
      "source": [
        "# FUNCTION CALL FOR DATA CLEANING AND PREPARATION #\n",
        "utils = Utils()\n",
        "X2,y_test=utils.readData2('tamil_sentiment_full_test_withtlabels.tsv')\n",
        "\n",
        "X_test=[]\n",
        "\n",
        "for i in range(0, len(X2)):\n",
        "    t = utils.cleanText(X2[i])\n",
        "    t = utils.remove_emoticon(t)\n",
        "    t = utils.remove_punc(t)\n",
        "    t = utils.remove_stops(t)\n",
        "    t = utils.lemmatization(t)\n",
        "    X_test.append(t)\n",
        "#print(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqPVciOg-K5c"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC, SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhcrBCgJLDub"
      },
      "source": [
        "line = ['', 'precision', 'recall', 'f1-score',  'support']\n",
        "df = pd.DataFrame(line).transpose()\n",
        "csv_data = df.to_csv(index=False)\n",
        "df.to_csv('Tamil.csv',  mode='a', header=False, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODSXTCHsLd8Q",
        "outputId": "b95aacc3-45df-48b8-de05-7ddb81abb733"
      },
      "source": [
        "# LOGISTIC REGRESSION #\n",
        "lrp = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2,analyzer='word', ngram_range=(1, 3))),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('lr', LogisticRegression(max_iter=1000))\n",
        "                ])\n",
        "lrp.fit(X_train, y_train)\n",
        "y_pred = lrp.predict(X_test)\n",
        "\n",
        "# prediction results\n",
        "#utils.draw_prediction_results(y_pred,y_test,ourTags,\"Logistic Regression\")\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'LR')\n",
        "\n",
        "#prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Logistic Regression\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Logistic Regression: 0.6442526124488869\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.358108  0.112766  0.171521       470\n",
            " unknown_state   0.477663  0.291405  0.361979       477\n",
            "      Positive   0.683634  0.907306  0.779747      2546\n",
            "Mixed_feelings   0.757812  0.397541  0.521505       244\n",
            "      Negative   0.519737  0.356391  0.422837       665\n",
            "\n",
            "      accuracy                       0.644253      4402\n",
            "     macro avg   0.559391  0.413082  0.451518      4402\n",
            "  weighted avg   0.605911  0.644253  0.601306      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGB5YURAThfx",
        "outputId": "65a8ad4a-4f5d-4699-fd8d-9912765183db"
      },
      "source": [
        "# MULTINOMIAL NAIVE BAYES #\n",
        "multinomial_naive_bayes = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('multinomial_naive_bayes',\n",
        "                         MultinomialNB())\n",
        "                        ])\n",
        "multinomial_naive_bayes.fit(X_train, y_train)\n",
        "y_pred = multinomial_naive_bayes.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'MNB')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Multinomial Naive Bayes\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Multinomial Naive Bayes: 0.6256247160381645\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.437500  0.029787  0.055777       470\n",
            " unknown_state   0.500000  0.132075  0.208955       477\n",
            "      Positive   0.627706  0.968185  0.761625      2546\n",
            "Mixed_feelings   0.839286  0.385246  0.528090       244\n",
            "      Negative   0.575610  0.177444  0.271264       665\n",
            "\n",
            "      accuracy                       0.625625      4402\n",
            "     macro avg   0.596020  0.338548  0.365142      4402\n",
            "  weighted avg   0.597417  0.625625  0.539352      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22IIDKIOwTaK",
        "outputId": "76e80a27-fb43-45a8-e87e-68dbb6b34c3c"
      },
      "source": [
        "# LINEAR SVM #\n",
        "linear_svm = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('linear_svc',\n",
        "                        SVC(kernel='linear'))\n",
        "                        ])\n",
        "linear_svm.fit(X_train, y_train)\n",
        "y_pred = linear_svm.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'L-SVM')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Linear SVM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Linear SVM: 0.6367560199909132\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.389706  0.112766  0.174917       470\n",
            " unknown_state   0.460317  0.303983  0.366162       477\n",
            "      Positive   0.678603  0.893166  0.771240      2546\n",
            "Mixed_feelings   0.743243  0.450820  0.561224       244\n",
            "      Negative   0.488938  0.332331  0.395703       665\n",
            "\n",
            "      accuracy                       0.636756      4402\n",
            "     macro avg   0.552162  0.418613  0.453849      4402\n",
            "  weighted avg   0.599035  0.636756  0.595304      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1cYONgUxcBy",
        "outputId": "31ab230f-2a5b-4cf2-feb3-0f1f7964f892"
      },
      "source": [
        "# RBF SVM #\n",
        "rbf_svm = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('rbf_svc',\n",
        "                        SVC(kernel='rbf', gamma=1))\n",
        "                        ])\n",
        "rbf_svm.fit(X_train, y_train)\n",
        "y_pred = rbf_svm.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'R-SVM')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"RBF SVM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of RBF SVM: 0.6369831894593366\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.500000  0.061702  0.109848       470\n",
            " unknown_state   0.481651  0.220126  0.302158       477\n",
            "      Positive   0.650315  0.932050  0.766102      2546\n",
            "Mixed_feelings   0.746667  0.459016  0.568528       244\n",
            "      Negative   0.565749  0.278195  0.372984       665\n",
            "\n",
            "      accuracy                       0.636983      4402\n",
            "     macro avg   0.588876  0.390218  0.423924      4402\n",
            "  weighted avg   0.608555  0.636983  0.575422      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-ucCcstztRa",
        "outputId": "f0b27f26-8953-4ccf-89a4-3ab7e03e47cc"
      },
      "source": [
        "# POLY SVM #\n",
        "poly_svm = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('poly_svc',\n",
        "                        SVC(kernel='poly',degree = 1))\n",
        "                        ])\n",
        "poly_svm.fit(X_train, y_train)\n",
        "y_pred = poly_svm.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'P-SVM')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"POLY SVM\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of POLY SVM: 0.6365288505224898\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.384058  0.112766  0.174342       470\n",
            " unknown_state   0.460317  0.303983  0.366162       477\n",
            "      Positive   0.678710  0.892773  0.771162      2546\n",
            "Mixed_feelings   0.743243  0.450820  0.561224       244\n",
            "      Negative   0.488938  0.332331  0.395703       665\n",
            "\n",
            "      accuracy                       0.636529      4402\n",
            "     macro avg   0.551053  0.418535  0.453719      4402\n",
            "  weighted avg   0.598494  0.636529  0.595198      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5ovnOnrv6-Z",
        "outputId": "3d9f9267-8871-4ec1-b4fd-6c18835c7dc3"
      },
      "source": [
        "# RANDOM FOREST #\n",
        "random_forest = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('random_forest',\n",
        "                         RandomForestClassifier())\n",
        "                        ])\n",
        "random_forest.fit(X_train, y_train)\n",
        "y_pred = random_forest.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'RF')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Random Forest\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Random Forest: 0.6342571558382554\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.372549  0.040426  0.072937       470\n",
            " unknown_state   0.469880  0.245283  0.322314       477\n",
            "      Positive   0.654561  0.927337  0.767431      2546\n",
            "Mixed_feelings   0.642105  0.500000  0.562212       244\n",
            "      Negative   0.567213  0.260150  0.356701       665\n",
            "\n",
            "      accuracy                       0.634257      4402\n",
            "     macro avg   0.541261  0.394639  0.416319      4402\n",
            "  weighted avg   0.590553  0.634257  0.571624      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s31N5aEL8rIu",
        "outputId": "c96a03c9-1718-4f0f-9961-047348e898d8"
      },
      "source": [
        "# KNeighborsClassifier #\n",
        "knn = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('kNN', KNeighborsClassifier(n_neighbors=3))\n",
        "                        ])\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'kNN')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"KNeighborsClassifier\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of KNeighborsClassifier: 0.5615629259427533\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.114679  0.053191  0.072674       470\n",
            " unknown_state   0.219731  0.102725  0.140000       477\n",
            "      Positive   0.613517  0.873527  0.720791      2546\n",
            "Mixed_feelings   0.400000  0.204918  0.271003       244\n",
            "      Negative   0.587678  0.186466  0.283105       665\n",
            "\n",
            "      accuracy                       0.561563      4402\n",
            "     macro avg   0.387121  0.284166  0.297515      4402\n",
            "  weighted avg   0.501847  0.561563  0.497606      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc8SBEBsMeo8"
      },
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import StackingClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm7FWo_vNqbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15bf3ba4-2f15-4791-b19b-64d8a2240e4a"
      },
      "source": [
        "# EXTRA TREE CLASSIFIER #\n",
        "extra_tree = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('extra tree',\n",
        "                         ExtraTreesClassifier())\n",
        "                        ])\n",
        "extra_tree.fit(X_train, y_train)\n",
        "y_pred = extra_tree.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'XTree')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Extra Tree Classifier\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Extra Tree Classifier: 0.6310767832803271\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.305556  0.046809  0.081181       470\n",
            " unknown_state   0.452899  0.262055  0.332005       477\n",
            "      Positive   0.662944  0.909269  0.766810      2546\n",
            "Mixed_feelings   0.613861  0.508197  0.556054       244\n",
            "      Negative   0.533333  0.288722  0.374634       665\n",
            "\n",
            "      accuracy                       0.631077      4402\n",
            "     macro avg   0.513719  0.403010  0.422137      4402\n",
            "  weighted avg   0.579725  0.631077  0.575563      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCP0liD9P5Mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe44fd4-fe9a-429c-b2a1-1ebe4e478024"
      },
      "source": [
        "# VOTING CLASSIFIER #\n",
        "# ALL CLASSIFIERS #\n",
        "\n",
        "#create a dictionary of our models\n",
        "estimators=[(\"Linear SVM\", linear_svm), (\"Logistic Regression\", lrp), (\"Multinomial Naive Bayes\", multinomial_naive_bayes), (\"Extra Tree\", extra_tree), (\"Random Forest\", random_forest), (\"Poly SVM\", poly_svm), (\"RBF SVM\", rbf_svm), (\"KNeighborsClassifier\", knn)]\n",
        "\n",
        "hard_ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "hard_ensemble.fit(X_train, y_train)\n",
        "y_pred = hard_ensemble.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'HEns')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Hard Ensemble\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Hard Ensemble: 0.6367560199909132\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.465753  0.072340  0.125230       470\n",
            " unknown_state   0.487603  0.247379  0.328234       477\n",
            "      Positive   0.650082  0.934014  0.766602      2546\n",
            "Mixed_feelings   0.795455  0.430328  0.558511       244\n",
            "      Negative   0.565657  0.252632  0.349272       665\n",
            "\n",
            "      accuracy                       0.636756      4402\n",
            "     macro avg   0.592910  0.387339  0.425570      4402\n",
            "  weighted avg   0.608099  0.636756  0.576042      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtugxjUBOlbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51aea836-6193-490b-94a1-641a035f5f59"
      },
      "source": [
        "# VOTING CLASSIFIER #\n",
        "# TOP 5 CLASSIFIERS #\n",
        "\n",
        "#create a dictionary of our models\n",
        "estimators=[(\"Linear SVM\", linear_svm), (\"Logistic Regression\", lrp),  (\"Random Forest\", random_forest), (\"Poly SVM\", poly_svm), (\"RBF SVM\", rbf_svm)]\n",
        "\n",
        "hard_ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "hard_ensemble.fit(X_train, y_train)\n",
        "y_pred = hard_ensemble.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'HEns5')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Hard Ensemble\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Hard Ensemble: 0.641753748296229\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.429825  0.104255  0.167808       470\n",
            " unknown_state   0.483986  0.285115  0.358839       477\n",
            "      Positive   0.670809  0.911626  0.772894      2546\n",
            "Mixed_feelings   0.773723  0.434426  0.556430       244\n",
            "      Negative   0.519512  0.320301  0.396279       665\n",
            "\n",
            "      accuracy                       0.641754      4402\n",
            "     macro avg   0.575571  0.411145  0.450450      4402\n",
            "  weighted avg   0.607684  0.641754  0.594529      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2WVBt7tQ0MA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8a1db45-0b9d-4e6d-8fe4-c55f0d71893e"
      },
      "source": [
        "# VOTING CLASSIFIER #\n",
        "# TOP 3 CLASSIFIERS #\n",
        "\n",
        "#create a dictionary of our models\n",
        "estimators=[(\"Linear SVM\", linear_svm), (\"Logistic Regression\", lrp), (\"Poly SVM\", poly_svm)]\n",
        "\n",
        "hard_ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "hard_ensemble.fit(X_train, y_train)\n",
        "y_pred = hard_ensemble.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'HEns3')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Hard Ensemble\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Hard Ensemble: 0.6367560199909132\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.386861  0.112766  0.174629       470\n",
            " unknown_state   0.460317  0.303983  0.366162       477\n",
            "      Positive   0.678806  0.893166  0.771370      2546\n",
            "Mixed_feelings   0.743243  0.450820  0.561224       244\n",
            "      Negative   0.488938  0.332331  0.395703       665\n",
            "\n",
            "      accuracy                       0.636756      4402\n",
            "     macro avg   0.551633  0.418613  0.453818      4402\n",
            "  weighted avg   0.598849  0.636756  0.595349      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tevtGnJ6RIYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb49c43-0456-409a-c76a-d678dee29228"
      },
      "source": [
        "# VOTING CLASSIFIER #\n",
        "# BEST OF ALL CLASSIFIERS #\n",
        "\n",
        "#create a dictionary of our models\n",
        "estimators=[(\"Linear SVM\", linear_svm), (\"Logistic Regression\", lrp), (\"Multinomial Naive Bayes\", multinomial_naive_bayes), (\"Extra Tree\", extra_tree)]\n",
        "\n",
        "hard_ensemble = VotingClassifier(estimators, voting=\"hard\")\n",
        "hard_ensemble.fit(X_train, y_train)\n",
        "y_pred = hard_ensemble.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'HEnsA')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Hard Ensemble\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Hard Ensemble: 0.6406179009541118\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.428571  0.102128  0.164948       470\n",
            " unknown_state   0.491289  0.295597  0.369110       477\n",
            "      Positive   0.658863  0.928515  0.770786      2546\n",
            "Mixed_feelings   0.790323  0.401639  0.532609       244\n",
            "      Negative   0.580756  0.254135  0.353556       665\n",
            "\n",
            "      accuracy                       0.640618      4402\n",
            "     macro avg   0.589960  0.396403  0.438202      4402\n",
            "  weighted avg   0.611604  0.640618  0.586343      4402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrvYzJMSSl2H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ec9113f-afae-449b-a4f0-c729eeb295c3"
      },
      "source": [
        "# ADABOOST #\n",
        "#seed = 10\n",
        "num_trees = 25\n",
        "\n",
        "ada_boost = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('ada_boost',\n",
        "                         AdaBoostClassifier(n_estimators=num_trees))\n",
        "                        ])\n",
        "ada_boost.fit(X_train, y_train)\n",
        "y_pred = ada_boost.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'AdaB')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"Ada Boost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of Ada Boost: 0.5860972285324852\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.000000  0.000000  0.000000       470\n",
            " unknown_state   0.000000  0.000000  0.000000       477\n",
            "      Positive   0.608178  0.923016  0.733229      2546\n",
            "Mixed_feelings   0.654206  0.286885  0.398860       244\n",
            "      Negative   0.372093  0.240602  0.292237       665\n",
            "\n",
            "      accuracy                       0.586097      4402\n",
            "     macro avg   0.326895  0.290101  0.284865      4402\n",
            "  weighted avg   0.444227  0.586097  0.490337      4402\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai2QQGdioGlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4987c653-f6a2-4b4d-91a0-d30eb68a0c2a"
      },
      "source": [
        "# XGBOOST #\n",
        "xg_boost = Pipeline([('vect', CountVectorizer(min_df=3, max_df=0.2, analyzer='word', ngram_range=(1, 3),)),\n",
        "                        ('tfidf', TfidfTransformer()),\n",
        "                        ('xgboost',\n",
        "                         XGBClassifier(learning_rate =0.1,\n",
        " n_estimators=1000,\n",
        " max_depth=5,\n",
        " min_child_weight=1,\n",
        " gamma=0,\n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " nthread=4,\n",
        " scale_pos_weight=1,\n",
        " seed=27))\n",
        "])\n",
        "xg_boost.fit(X_train, y_train)\n",
        "y_pred = xg_boost.predict(X_test)\n",
        "\n",
        "utils.create_csv(y_pred, y_test, 'XGB')\n",
        "\n",
        "# prediction results\n",
        "utils.draw_prediction_results(y_pred,y_test,ourTags,\"XGBoost\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of XGBoost: 0.6285779191276692\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     not-Tamil   0.324138  0.100000  0.152846       470\n",
            " unknown_state   0.460145  0.266247  0.337317       477\n",
            "      Positive   0.673015  0.902200  0.770935      2546\n",
            "Mixed_feelings   0.658065  0.418033  0.511278       244\n",
            "      Negative   0.469734  0.291729  0.359926       665\n",
            "\n",
            "      accuracy                       0.628578      4402\n",
            "     macro avg   0.517019  0.395642  0.426460      4402\n",
            "  weighted avg   0.581161  0.628578  0.581472      4402\n",
            "\n"
          ]
        }
      ]
    }
  ]
}